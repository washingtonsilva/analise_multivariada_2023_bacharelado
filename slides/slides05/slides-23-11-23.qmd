---
title: "Análise Multivariada"
subtitle: "Bacharelado em Admnistração"
author: "Prof. Washington Santos da Silva"
institute: "IFMG - Campus Formiga"
date: today
date-format: long
lang: "pt"
format: 
  revealjs:
    slide-number: true
    progress: true
    incremental: false
    transition: slide
    code-link: true
    self-contained: true
    preview-links: false
    chalkboard: false
    overview: true
    logo: img/logo.jpeg
    css: logo.css
editor: source
crossref:
  fig-title: '**Fig.**'
  fig-labels: arabic
  title-delim: "**.**"
execute: 
  echo: true
bibliography: referencias.bib
csl: associacao-brasileira-de-normas-tecnicas.csl
---


## Visão Geral

Esta apresentação é principalmente uma introdução *conceitual* à análise 
fatorial.

Vamos nos concentrar mais em conceitos e melhores práticas do que em detalhes técnicos:

- Motivação e noções básicas
- Análise Fatorial Exploratória (*Exploratory Factorial Analysis*, *EFA*)



## Motivação

- A Análise Fatorial (FA) é uma família de técnicas para avaliar 
a presença de construtos (conceitos) em pesquisas de mercado, avaliações psicológicas...

- **Fatores** são considerados **variáveis latentes**, variáveis que não 
podem ser observadas diretamente, mas que podem ser imperfeitamente medidas através da sua relação com outras variáveis.

- Podemos identificar exemplos canônicos de fatores em testes psicológicos e educacionais.



## Motivação

- Por exemplo, “**inteligência**” e “**ansiedade**” são conceitos abstratos 
(construtos) que não são diretamente observáveis.

- No entanto, considera-se que eles podem ser observados empiricamente 
através de múltiplos comportamentos, cada um dos quais é um indicador 
imperfeito da variável latente subjacente.

- Estes valores observados são conhecidos como *variáveis manifestas* e 
incluem indicadores como pontuações de testes, pesquisas
respostas e outros comportamentos empíricos.



## Motivação

- A Análise Fatorial Exploratória (EFA) busca encontrar o grau em que 
fatores latentes, são responsáveis pela variância observada
das variáveis manifestas.

- Em marketing, frequentemente observa-se um grande número de variáveis que, 
acredita-se, poderiam estar relacionadas a um conjunto menor de construtos subjacentes. 



## Motivação

- Por exemplo, não podemos observar diretamente a **satisfação do cliente**, 
mas podemos observar respostas em pesquisas que incluem perguntas sobre
diferentes aspectos da experiência de um cliente, representando conjuntamente diferentes facetas do construto subjacente, satisfação. 

- Da mesma forma, não podemos observar diretamente a **intenção de compra**, 
**sensibilidade ao preço** ou **engajamento**, mas é possível observar vários
comportamentos que estão relacionados a esses construtos.



## Motivação: Pesquisa em um mercado local

- O dono do estabelecimento gostaria de entender o que faz com que os clientes visitem o local uma vez e não voltem, preferindo comprar na concorrência.

- Para isso, ele realiza uma pesquisa qualitativa e constata que os clientes reclamam de vários fatores, tais como atendentes de mau humor, o aspecto das verduras, produtos que estão quase chegando na data de validade, formas de pagamento disponíveis dentre outras reclamações.



## Motivação: Pesquisa em um mercado local {.scrollable}

- Ao final da pesquisa, ele percebe que possui mais de cinquenta tipos de reclamações diferentes, e simplesmente não tem tempo para analisar como cada uma delas influencia na sua perda de vendas. 

- O que fazer? É hora de fazer uma pesquisa quantitativa e utilizar a análise fatorial exploratória.

- Após aplicar um questionário medindo a satisfação dos clientes em relação aos mais de cinquenta tipos de reclamações, utilizou a análise fatorial e verificou que havia correlação entre algumas reclamações.

- Simpatia dos funcionários, rapidez no atendimento, conhecimento dos funcionários, cortesia, etc.;

- Estado da pintura do mercado, limpeza, disposição dos produtos, etc.;
Marcas e produtos disponíveis, preço dos produtos, formas de pagamento, etc.


## Motivação: Pesquisa em um mercado local

![Fatores identificados na pesquisa](img/fatores.jpeg){#fig-fator width=70% fig-align='center'}





## O Conceito Geral - Versão 1 {.smaller}

- A partir das variáveis originais, a análise fatorial (FA) busca encontrar
um número menor de variáveis derivadas (*fatores*) que atendem às seguintes
condições:

1. Capturem ao máximo as correlações entre as variáveis originais 
   (após contabilizar o erro).
2. Cada fator está claramente associado a um subconjunto de variáveis.
3. Cada variável está claramente associada (idealmente) a apenas um fator.
4. Os fatores são diferenciados ao máximo uns dos outros.

- Estas condições raramente são verificadas perfeitamente na prática.

- Entretanto, é possível obter soluções aproximadas, que podem fornecer uma 
estrutura de fatores "simples" e interpretável.



## Exemplo da Versão 1 {.smaller}

Considere a análise fatorial (fictícia) de um teste escolar padronizado:

  Variável                  | Fator 1  | Fator 2  | Fator 3  
--------------------------:|:--------:|:--------:|:--------:
Pontuação aritmética        | 0,45     | **0,88** | 0,25
Pontuação álgebra           | 0,51     | **0,82** | 0,03
Pontuação lógica            | 0,41     | 0,50     | 0,11
Pontuação quebra-cabeça     | 0,25     | 0,42     | 0,07
Pontuação vocabulário       | 0,43     | 0,09     | **0,93**
Pontuação leitura           | 0,50     | 0,14     | **0,85**

Podemos interpretar os fatores produzidos como:

- Fator 1: aptidão geral
- Fator 2: habilidades matemáticas
- Fator 3: competências linguísticas



## O Conceito Geral - Versão 2 {.smaller}

- Outra maneira de olhar para a FA é que ela busca estimar 
  *variáveis latentes*. 

- Uma variável latente é um processo de geração de dados não observável --- 
como um estado mental  --- que se manifesta em quantidades mensuráveis, como 
itens de pesquisas.

- Um artigo propos uma escala para avaliar o interesse em um produto, essa 
escala foi projetada para avaliar três variáveis latentes:

- Interesse geral em uma categoria de produto
- Interesse detalhado em características específicas
- Interesse no produto como um produto de “imagem”

- Busca-se mensurar uma variável latente avaliando vários itens 
relacionados (manifestações), uma vez que qualquer item único, muito 
provavelmente irá capturar a variável latente de forma imperfeita.



## Exemplo da Versão 2

```{r, echo = FALSE, results ="hide", message = FALSE, warning = FALSE }
# specifica o modelo
piesDataModel <- "Geral =~ 0.9*i1 + 0.7*i2 + 0.5*i3
                  Caracteristica =~ 0.3*i3 + 0.7*i4 + 0.9*i5 + 0.5*i6  + 0.9*i7
                  Imagem =~ 0.2*i3 + 0.8*i8 + 0.9*i9 + 0.8*i10 + 0.7*i11 
                  PIES =~ 0.7*General + 0.8*Feature + 0.8*Image"

# dados simulados
library(lavaan)
set.seed(10001)    
piesSimData.norm <- simulateData(piesDataModel, sample.nobs = 3600)
print(head(piesSimData.norm), digits = 2)

# converter os números reais em itens do tipo Likert na escala de 1 a 7
piesSimData <- data.frame(lapply(piesSimData.norm, 
                          function(x) { cut(x, breaks = 7, labels = FALSE) } ))

# ajusta o modelo estrutural aos dados simulados
# definir o modelo estrutural hierárquico 
piesModel0 <- "Geral =~ i1 + i2 + i3
               Caracteristica =~ i4 + i5 + i6  + i7
               Imagem =~ i8 + i9 + i10 + i11"

pies.fit <- cfa(piesModel0, data = piesSimData, warn = FALSE)

# figura do modelo
library(semPlot)
semPaths(pies.fit, what = "path", fade = FALSE, residuals = FALSE,
         edge.label.cex = 0.75, fixedStyle = 1, edge.color = "darkblue")
```



## Analise Fatorial {.scrollable}

### Modelo Fatorial: 1 fator comum

$$
\begin{align}
X_1 &= \lambda_{1}f_1 + \epsilon_1 \\
X_2 &= \lambda_{2}f_1 + \epsilon_2 \\
X_3 &= \lambda_{3}f_1 + \epsilon_3 
\end{align}
$$

- O fator $f$ não é observado, somente $X_1$, $X_2$, $X_3$ são observadas.

- $\epsilon_i$ representa a variabilidade de $X_i$ não explicada por $f$.

- $X_i$ é uma função linear de $f$ e $\epsilon$


### Modelo Fatorial: 2 fatores comuns

Um modelo com 2 fatores pode ser representado por:

$$
\begin{align}
X_1 &= \lambda_{11}f_1 + \lambda_{12}f_2 + \epsilon_1 \\
X_2 &= \lambda_{21}f_1 + \lambda_{22}f_2 + \epsilon_2 \\
X_3 &= \lambda_{31}f_1 + \lambda_{32}f_2 + \epsilon_3 \\
X_4 &= \lambda_{41}f_1 + \lambda_{42}f_2 + \epsilon_4 \\
\end{align}
$$

Generalizando para $p$ observações e $k$ fatores, temos:

$$
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_p
\end{bmatrix}_{p \times 1} = 
\begin{bmatrix}
    \lambda_{11} & \lambda_{12} & \dots & \lambda_{1k} \\
    \lambda_{21} & \lambda_{22} & \dots & \lambda_{2k} \\
    \vdots & \vdots & \ddots & \vdots \\
    \lambda_{p1} & \lambda_{p2} & \dots & \lambda_{pk}
\end{bmatrix}_{p \times k} 
\begin{bmatrix}
f_1 \\
f_2 \\
\vdots \\
f_k
\end{bmatrix}_{k \times 1} + 
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_p
\end{bmatrix}_{p \times 1}
$$

### Notação Matricial 
$$
X = \Lambda F + \epsilon
$$
sendo:

- $X = (X_1, X_2,\ldots,X_p)^{'}$ um vetor aleatório com vetor de médias 
$\mu$ e matriz de covariância $\Sigma$.

- $\Lambda = \{\lambda_{ij}\}_{p \times k}$ é a matriz ($p \times k$) das cargas 
fatoriais.  

    - $\lambda_{ij}$ é a carga da i-ésima variável em relação ao $j-$ésimo fator.

- $F = (f_1, f_2,\ldots,f_k)^{'}$ é um vetor de scores fatoriais.

- $\epsilon = (\epsilon _1, \epsilon _2,\ldots,\epsilon_p)^{'}$ é um vetor de 
erros.


### 2. Hipóteses

- $F \sim (0, I_k)$, i.e, Os fatores latentes têm média zero, variância 
unitária e são não correlacionados.

- $\epsilon \sim (0, \varepsilon_k)$, sendo $\varepsilon = diag(\epsilon _1, \epsilon _2,\ldots,\epsilon_p)^{'}$ com $\epsilon_j$ denotando a j-ésima 
variância específica ou singularidade.

- $\epsilon_j$ e $f_k$ são independentes para todo par $j,k$.

- Nenhum dos componentes além de $x$ é observado, mas a principal restrição 
é que os fatores não sejam correlacionadas e tenham média zero e variância unitária, e que os erros sejam independentes com variância $\epsilon$.


### 3. Estrutura de Covariância Implícita de X

$$
\begin{align*}
V(X) = \Sigma &= E[(X - \mu)(X - \mu)^{'}], \\
     &= E[(\Lambda F + \epsilon)(\Lambda F + \epsilon)^{'}], \\
     &= E[\Lambda F F^{'} \Lambda^{'}] + E[\Lambda F \epsilon^{'}] + 
        E[\epsilon F^{'} \Lambda^{'}] + E[\epsilon \epsilon^{'}], \\
     &= \Lambda E[FF^{'}] \Lambda^{'} + \Lambda E[F\epsilon^{'}] + 
         E[\epsilon F^{'}]\Lambda^{'} + E[\epsilon \epsilon^{'}], \\
\Sigma &= \Lambda \Lambda^{'} + \varepsilon
\end{align*}
$$

- Visto que: $E[FF^{'}] = I_k$, $E[F\epsilon^{'}] = 0_{k \times p}$, 
$E[\epsilon F^{'}] = 0_{p \times k}$ e $E[\epsilon \epsilon^{'}] = \varepsilon$.

- Isto implica que a covariância entre $X$ e $F$ é:

$$
\begin{align*}
Cov(X,F) &=  E[(X - \mu)F^{'}], \\
         &= E[(\Lambda F + \epsilon)F^{'}], \\
         &= \Lambda
\end{align*}
$$

- Assim, podemos notar que o modelo da análise fatorial é em essência um 
modelo para a matriz de correlação de $x$.


### 4. Variância Explicada pelos Fatores Comuns

A porção da variância da $i-$ésima variável que é explicada pelos k 
fatores comuns é chamada de comunalidade da i-ésima variável:

$$
\underbrace{\sigma_{ii}}_{V(X_i)} = \underbrace{h_{i}^2}_{\text{communality}} + \underbrace{\varepsilon_i}_{\text{uniqueness}}
$$
sendo:

- $\sigma_{ii}$ a variância de $X_i$

- $h_{i}^2 = (\Lambda \Lambda^{'})_{ii} = \lambda_{i1}^2 + \lambda_{i2}^2 + \ldots + \lambda_{ik}^2$ é a *communality* de $X_i$.

- $\varepsilon_i$ =  é a variância específica, ou *uniqueness*, de $X_i$.

- Note que a *communality* $h_{i}^2$ é a soma das cargas quadráticas de $X_i$.


### Estimação por Máxima Verossimilhança do Modelo Fatorial 

- Note que os parâmetros de interesse são as cargas fatoriais ($\Lambda$) e 
as variâncias específicas (*uniqueness*) da diagonal de $\varepsilon$.

- Suponha que $X \overset{\text{i.i.d.}}{\sim} N(\mu, \Lambda \Lambda^{'} + \varepsilon)$ seja um vetor normal multivariado.

- A função log-verossimilhança para uma amostra de n observações tem a forma:

$$
\log L_n(\mu, \Lambda, \varepsilon;x) = \frac{np\log(2\pi)}{2} + 
                                        \frac{n\log(|\Sigma^{-1}|)}{2} + 
                \frac{\sum_{i=1}^{n} (x_i - \mu)^{'} \Sigma^{-1}(x_i - \mu))}{2}
$$

- sendo $\hat{\Sigma} = \hat{\Lambda} \hat{\Lambda}^{'} + \hat{\varepsilon}$.

- Utilizamos um algoritmo iterativo para maximizar $\log L_n(\mu, \Lambda,\varepsilon;x) $.

- Este é o método implementado na função `factanal()` da linguagem R 

- Uma outra função muito utilizada é `fa()` do pacote psych. Esta função 
possui diversos métodos possíveis de estimação programados:

    - Via Componentes Principais/Decomposição em Valor Singular (SVD)
    - Mínimos Quadrados Ordinários
    - Método Iterativo de Fatoração do Eixo Principal
    - ...
    

### Solução Indeterminada 

Há uma indeterminação no modelo, pois ele permanece inalterado se 
$\Lambda$ for substituída por $G\Lambda$ para qualquer matriz 
ortogonal $G$. 

Estas matrizes $G$ são conhecidas como **rotações**, embora o termo também 
seja aplicado a matrizes invertíveis não ortogonais.



## Tipos de Análise Fatorial {.scrollable}

#### Análise Fatorial Exploratória (EFA)

- Pergunta quais são os fatores presentes nos dados observados
- Requer interpretação
- Antes de assumir que está correto, necessário confirmar com a CFA

#### Análise Fatorial Confirmatória (CFA)

- Pergunta quão bem um modelo proposto *se ajusta* a determinados dados
- É um tipo de Modelo de Equações Estruturais (SEM)
- Não fornece uma resposta absoluta; é necessário comparar modelos



## Termos e símbolos principais {.scrollable}

- **Variável latente**: um suposto processo cognitivo ou de geração de dados que
leva a dados observáveis. Muitas vezes é uma *construção* teórica. Exemplo:
*Interesse no produto*. Símbolo: círculo/oval.

- **Variável manifesta**: variáveis observáveis que expressam variáveis latentes.
Exemplo: "Quão interessado você está neste produto?" Símbolo: caixa.
  
- **Fator**: estimativa de uma variável latente e de seu relacionamento com variáveis manifestas. Exemplo: Interesse geral em uma 
categoria de produto.

- **Cargas**: a força do relacionamento entre um fator e uma variável.
Exemplo: *F1 $\rightarrow$ v1 = 0,45*. Intervalo [-1,0 ... 1,0], 
iguais ao *r* de Pearson.



## Modelo Fatorial: Visualização

```{r, echo=FALSE, results="hide", message=FALSE, warning=FALSE }
library(semPlot)
semPaths(pies.fit, what = "path", fade = FALSE, residuals = FALSE,
         edge.label.cex = 0.75, fixedStyle = 1, edge.color = "darkblue")
```



## Etapas para a EFA {.scrollable}

1. Importar e preparar os dados (padronização e lidar com assimetrias).
   
2. Examinar a matriz de correlação para ter uma ideia dos possíveis fatores.

3. **Determinar o número de fatores**.

4. **Escolher um método de estimação**.

5. **Escolher uma rotação dos fatores**.

6. Estimar o modelo e interpretar os fatores resultantes.

7. Repetir as etaps 3-5 se os resultados não forem claros, selecionar o 
modelo mais útil.

  

## Agora ... dados!

- O arquivo `brand.csv` contém dados típicos de pesquisas sobre **percepção de marcas** (brand perception survey).

- Esses dados refletem as avaliações dos consumidores sobre as marcas em 
relação aos adjetivos ou atributos perceptivos expressos nos itens 
da pesquisa.

- Uma questão desse tipo de pesquisa possui, em geral, a seguinte forma:

*Em uma escala de 1 a 10 – onde 1 é o mínimo e 10 é o máximo – quão [ADJETIVO ou ATRIBUTO] é a [MARCA A]?*



## Brand perception survey

- Nos dados produzidos por este tipo de pesquisa, uma observação (linha) é a avaliação que um entrevistado faz de uma marca em um dos adjetivos. 
Dois exemplos de questões deste tipo são:

- Quão última moda (`trendy`) é o Café Dona Maria?

- Quão líder de categoria (`leader`) é o Café Três Coracões?



## Brand perception survey

- Os dados envolvem classificações simuladas de 10 marcas (*brands*) 
(nomeadas como `a` a `j`) em 9 adjetivos, para $N = 100$ entrevistados.

- Os 9 adjetivos são: `perform`, `leader`, `latest`, `fun`, `serious`, 
`bargain`, `value`, `trendy` e `rebuy`.



## Brand perception survey

::: {.panel-tabset}

### Importação dos dados

```{r}
url <- "http://goo.gl/IQl8nc"
brand_ratings <- read.csv(url)
head(brand_ratings)
```

### Estrutura

```{r}
dplyr::glimpse(brand_ratings)
```

:::



## Brand perception survey

Estatísticas Descritivas

```{r}
summary(brand_ratings)
```



## Brand perception survey

Padronizando as primeiras 9 colunas:

```{r}
brand_df <- brand_ratings
brand_df[, 1:9] <- scale(brand_ratings[, 1:9])
summary(brand_df)
```



## Análise Fatorial Exploratória {.scrollable}

Para determinarmos o número de fatores, podemos visualizar a matriz de 
correlações:

```{r}
#| fig-cap: "Podemos visualizar a formação de 3 clusters envolvendo variáveis similares, o primeiro envolve `fun`, `latest` e `trendy`, o segundo `rebuy`, `bargain` e `value`, e o terceiro `perform`, `leader` e `serious`."

corrplot::corrplot(cor(brand_df[ , 1:9]) , order = "hclust")
```



## Análise Fatorial Exploratória

- Outra possibilidade para determinarmos o número de fatores é usar um *Scree Plot*.

- Neste caso, retemos o número de fatores que apresentam o autovalor (uma métrica para proporção de variância explicada) maior que 1,0. 

- Um autovalor igual a 1,0 corresponde à quantidade de variância que pode ser atribuída a uma única variável manifesta. Assim, um fator que capture menos variância do que tal item pode ser considerado relativamente sem importância.



## Análise Fatorial Exploratória

*Scree Plot*

```{r}
nFactors::nScree(brand_df[, 1:9])
```

- A função `nScree()` do pacote `nFactors` aplica vários métodos para estimar o número de fatores e, no presente caso, três dos quatro métodos sugerem 
utilizarmos 3 fatores para modelar os dados.



## Análise Fatorial Exploratória {.scrollable}

- Podemos examinar os autovalores aplicando a função `eigen()` a uma matriz de correlação:

```{r}
eigen(cor(brand_df[, 1:9]))
```

- Os três primeiros autovalores são maiores que 1,0, embora o terceiro valor 
dificilmente seja.

- Isto novamente sugere 3 ou, talvez, 2 fatores.



## Análise Fatorial Exploratória

- A escolha de um modelo final depende da sua "interpretabilidade". 

- Para a EFA, uma prática recomendada é testar algumas soluções fatoriais, 
incluindo aquelas sugeridas pelos resultados do scree plot e dos autovalores

- Assim, testaremos uma solução com 2 fatores e outra com 3 fatores para ver 
qual delas é mais "útil".

- Podemos estimar o modelo fatorial com a função `factanal(x, factors = K)`, 
sendo $K$ o número de fatores a serem ajustados. 



## Modelo Fatorial: 2 fatores

```{r}
fa_brand <- factanal(brand_df[, 1:9], factors = 2, rotation = "varimax")
fa_brand
```


## `factanal`: Compreendendo os resultados {.scrollable}

### `Uniquenesses`

- A primeiro parte fornece as estimativas das "Uniquenesses", 
que variam de 0 a 1. São as estimativas $\hat{\varepsilon}$ do modelo estatístico.

- Ficamos atentos a altas singularidades. Uma alta singularidade para uma variável pode significar que ela não se encaixa perfeitamente em nossos fatores. 

- Por exemplo, `latest` possui $\hat{\varepsilon} = 0.796$ 

- Se subtrairmos as singularidades de 1, obteremos a estimativa da 
**comunalidade**, 1 - 0.796 = 0.204, que implica que 20,4% da variância 
de `latest` é explicada pelos dois fatores comuns.

- Em geral, deseja-se baixas singularidades ou altas comunalidades.

- **Nota**: A estimativa de máxima verossimilhança pode implicar em 
singularidades iguais a zero ou negativos. Estes são chamados de casos Heywood.


### `Loadings`

- A próxima seção contém as "Cargas", que variam de -1 a +1. Estas constituem 
$\hat{\Lambda}$. 

- Os fatores são ordenados em ordem decrescente das somas dos quadrados 
das cargas.

- O que estamos procurando são grupos de cargas altas. 

- Na primeira coluna, chamada `Factor1`, vemos que `bargain`, `value` e, um 
pouco `rebuy` possuem cargas altas em relação aos outros itens.

- Assim, poderíamos pensar nesse fator como relacionado a "valor".

- Observe que não há entrada para certos itens. Isso porque `factanal` 
não exibe cargas inferiores a 0,1. 

- Isso é para nos ajudar a identificar grupos de variáveis. O `cutoff = 0.1` 
pode ser alterado.

- O que são exatamente as cargas? Elas são simplesmente correlações com os 
fatores não observados. 

- `value` tem uma correlação de 0,873 com o Fator 1 e correlação 
insignificante com o outro fator. 

- Os valores precisos não são nossa principal preocupação. Estamos procurando 
grupos de correlações "altas" que esperamos que façam sentido e levem a um 
fator descritivo como "valor".

- O que constitui valores altos? Isso é difícil de dizer, que é uma das razões pelas quais a análise fatorial é tão subjetiva. 

- Não há um `cutoff` definido. O que você acha que é alto, outra pessoa pode pensar que é baixo.


### SS Loadings

- A próxima seção é tecnicamente parte das cargas. 

- Os números aqui resumem os fatores. A linha em que geralmente estamos mais interessados é a última, `Cumulative Var`. 

- Isso nos diz a proporção acumulada da variância explicada, então esses 
números variam de 0 a 1. Gostaríamos de ver um número final alto, onde mais 
uma vez "alto" é subjetivo. Nosso número final de 0,445 parece baixo.

- Parece que devemos considerar mais de dois fatores

- A linha acima, `Proportion Var`, é simplesmente a proporção da variância explicada por cada fator. 

- A linha `SS loadings` é a soma das cargas ao quadrado. Isso às vezes é usado para determinar o valor de um determinado fator. Dizemos que vale a pena manter um fator se a `SS loadings` for maior que 1. Em nosso exemplo, todas são 
maiores que 1.



## Teste de Hipótese {.scrollable}

- A última seção apresenta os resultados de um teste de hipótese. 

- A hipótese nula deste teste é que 2 fatores são suficientes para o nosso 
modelo. 

- O baixo valor-p nos leva a rejeitar a hipótese e considerar a adição de mais fatores. 

- Este teste de hipótese é possível devido ao método de estimação por máxima verossimilhança. 

- Lembre-se do modelo estimado da análise fatorial:

$$
\hat{\Sigma} = \hat{\Lambda} \hat{\Lambda}^{'} + \hat{\varepsilon}
$$

- Vamos ver o quão bem nosso modelo de dois fatores aproxima 
a matriz de correlação observada.

- Podemos extrair as cargas e singularidades do objeto `fa_brand` e executar a álgebra matricial necessária. 

- O operador `%*%` executa a multiplicação de matrizes. 

- A função `t()` transpõe uma matriz. 

- A função `diag()` pega um vetor de k números e cria uma matriz k x k com os números na diagonal e 0s em todos os outros lugares. 

- Abaixo, subtraímos a matriz de correlação estimada pelo modelo da 
matriz de correlação observada e arredondamos o resultado para 3 dígitos


```{r}
round(cor(as.matrix(brand_df[, 1:9])) - (fa_brand$loadings %*% t(fa_brand$loadings) + diag(fa_brand$uniquenesses)), 3)
```


- Esta é a `residual matrix`. 

- Gostaríamos de ver muitos números próximos de 0. Neste caso, temos vários números maiores que 0,05, indicando que nosso modelo de 2 fatores não é 
tão bom.



## Modelo Fatorial 2 fatores: Resumo

- As cargas fatoriais do Fator 1 com `bargain` e `value` são altas. Assim, 
podemos interpretá-lo como um fator relacionado a “valor”.

- O Fator 2 possui altas cargas fatoriais associadas com `leader` e `serious`. 
Podemos interpretá-lo como um fator relacionado a “líder de categoria”.

- Esta não é uma má interpretação, mas vamos compará-la a uma solução de 3 fatores.



## Modelo Fatorial: 3 fatores

```{r}
factanal(brand_df[, 1:9], factors = 3)
```



## Modelo Fatorial: 3 fatores {.scrollable}

- O modelo com 3 fatores explica uma proporção maior da variância 
do que o modelo com 2 fatores.

- Entretanto, o teste de hipótese de que 3 fatores são suficientes 
ainda apresenta um valor-p muito pequeno, o que nos leva a rejeitar a
hipótese de que 3 fatores são suficientes.

- A solução de 3 fatores mantém os fatores associados a “valor” e “líder de categoria”.

- Mas adiciona um claro fator associado a “mais recente” que possui altas 
cargas fatoriais com `latest` e `trendy`. 

- Isso adiciona um conceito interpretável à nossa compreensão dos dados. 

- Ele também se alinha com a maior parte das sugestões dos testes scree e autovalores.

- Portanto, consideramos o modelo com 3 fatores superior ao com 2 fatores, 
devido a PVE maior, mais consistentes com o número de fatores 
sugerido pelos dados e produz uma solução mais interpretável.


## Rotação Ortogonal {.scrollable}

Suponho que tenhamos $z = (x,y)^{'} \in \mathbb{R}^2$, i.e, pontos em um 
espaço euclidiano 2D.

Uma rotação ortogonal $2 \times 2$ de $(x,y)$ da forma:

$$
\begin{pmatrix} x^{\ast} \\ y^{\ast} \end{pmatrix} = 
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix}
$$

gira $(x,y)$ no sentido anti-horário em torno da origem por um ângulo $\theta$.

E uma rotação ortogonal $2 \times 2$ de $(x,y)$ da forma:

$$
\begin{pmatrix} x^{\ast} \\ y^{\ast} \end{pmatrix} = 
\begin{pmatrix}
\cos(\theta) & \sin(\theta) \\
-\sin(\theta) & \cos(\theta)
\end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix}
$$

gira $(x,y)$ no sentido horário em torno da origem por um ângulo $\theta$.

Note que a matriz de rotação $2 \times 2$

$$
R = 
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix}
$$

é uma matriz ortogonal para todo $\theta$:

$$
\begin{align}
R^{'}R &= 
\begin{pmatrix}
\cos(\theta) & \sin(\theta) \\
-\sin(\theta) & \cos(\theta)
\end{pmatrix}
\begin{pmatrix}
\cos(\theta) & -\sin(\theta) \\
\sin(\theta) & \cos(\theta)
\end{pmatrix} \\
    &= 
\begin{pmatrix}
\cos^2(\theta) + \sin^2(\theta)  & \cos(\theta)\sin(\theta) - \cos(\theta)\sin(\theta)\\
\cos(\theta)\sin(\theta) - \cos(\theta)\sin(\theta) & \cos^2(\theta) + \sin^2(\theta)
\end{pmatrix} \\
  &=
\begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}
\end{align}
$$
Suponha que temos uma matriz de dados $X$ com $p$ colunas:

- As linhas de $X$ são as coordenadas dos pontos no espaço p-dimensional

Uma rotação ortogonal $p \times p$ é uma transformação linear ortogonal

- $R^{'}R = RR^{'} = I_p$, sendo $I_p$ uma matriz identidade $p \times p$
- Se $\tilde{X} = XR$ é uma matriz de dados rotacionada, $\tilde{X}\tilde{X}^{'} = XX^{'}$.
- Rotações ortogonais preservam as relações entre os pontos.



## Indeterminação rotacional do Modelo de Análise Fatorial {.scrollable}

Suponha que $R$ seja uma matriz de rotação ortogonal, e note que:

$$
\begin{align}
X &= \Lambda F + \epsilon, \\
  &= \tilde{\Lambda} \tilde{F} + \epsilon
\end{align}
$$
Sendo:

- $\tilde{\Lambda} = \Lambda R$ as cargas fatoriais rotacionadas, e;
- $\tilde{F} = R^{'}F$ os scores fatoriais rotacionados

Temos que $\tilde{\Lambda} \tilde{\Lambda}^{'} = \Lambda \Lambda{'}$.

- Então podemos girar ortogonalmente a solução do modelo fatorial sem alterar a estrutura de covariância implícita.

- Os métodos de rotação fatorial deveriam tentar encontrar alguma rotação 
de uma solução de um modelo fatorial que forneça uma interpretação mais parcimoniosa.

- Entretanto, o que se observa muitas vezes, é que aplicam rotações buscando 
uma solução mais "interpretável".



## Rotação Ortogonal

```{r}
#| output-location: slide
#| fig-cap: "Rotação ortogonal no sentio horário"
rotmat2d <- function(theta) {
  matrix(c(cos(theta), sin(theta), -sin(theta), cos(theta)), 2, 2)
}
x <- seq(-2, 2, length = 11)
y <- 4 * exp(-x ^ 2) - 2
xy <- cbind(x, y)
rang <- c(30, 45, 60, 90, 180)
par(mfrow = c(2, 3))
plot(x,
     y,
     xlim = c(-3, 3),
     ylim = c(-3, 3),
     main = "Sem Rotacão")
text(x, y, labels = letters[1:11], cex = 1.5)
for (j in 1:5) {
  rmat <- rotmat2d(rang[j] * 2 * pi / 360)
  xyrot <- xy %*% rmat
  plot(xyrot, xlim = c(-3, 3), ylim = c(-3, 3))
  text(xyrot, labels = letters[1:11], cex = 1.5)
  title(paste(rang[j], " graus"))
}
```



## Rotações: Motivação {.scrollable}

- O resultado da EFA é simiar ao da PCA: uma matriz de fatores (semelhante a 
matriz de rotação da PCA) e sua relação com as variáveis originais (cargas dos fatores nas variáveis).

- Mas ao contrário da PCA, a EFA tenta encontrar soluções que sejam maximamente interpretáveis em termos das variáveis manifestas.

- Em geral, tenta encontrar soluções nas quais um pequeno número de cargas para 
cada fator sejam altas, enquanto outras cargas para esse fator sejam baixas.

- Quando isso é possível, esse fator pode ser interpretado em termos desse 
pequeno conjunto de variáveis manifestas.



## Rotações

- Para conseguir isso, a EFA usa **rotações** que começam com uma solução matemática não correlacionada (ortogonal) e depois alteram matematicamente a solução que explica a mesma prop. da variância, mas com cargas diferentes nas variáveis originais.

- Existem muitas rotações disponíveis e elas normalmente compartilham o 
objetivo de maximizar as cargas em algumas variáveis, ao mesmo tempo em que 
tornam os fatores tão distintos quanto possível uns dos outros.

- No entanto, algumas rotações são mais úteis do que outras porque ficam 
entre os itens grandes, em vez de dividi-los.



## Rotações {.scrollable}

- Isto posto, uma solução de análise fatorial pode ser rotacionada para ter novas cargas que representem a mesma proporção de variância. 

- Embora uma análise completa sobre rotações esteja fora do escopo deste 
curso, há uma questão que vale a pena considerar em qualquer EFA: **você deseja permitir que os fatores sejam correlacionados entre si ou não?**

- Podemos pensar que devemos deixar os dados decidirem. No entanto, a questão de permitir fatores correlacionados é menos uma questão sobre os dados do que 
sobre o conceito dos fatores latentes subjacentes.

- Consideramos que os fatores deveriam ser conceitualmente independentes ou faz mais sentido considerá-los relacionados? Uma rotação EFA pode ser obtida sob qualquer hipótese.



## Rotações {.scrollable}

- O padrão da função `factanal()` é encontrar fatores que tenham correlação 
zero (usando uma rotação `varimax`).

- Voltando aos nossos dados, podemos julgar que seria razoavelmente esperado 
que os fatores "valor" e "líder de categoria" estejam relacionados. 

- Em muitas categorias, o líder pode obter um prêmio no preço e, portanto, 
poderíamos esperar que essas duas construções latentes estivessem 
negativamente correlacionadas, em vez de serem independentes uma da outra.

- Isso sugere que poderíamos permitir fatores correlacionados na solução.

- A rotação a ser usado neste caso é a rotação **oblíqua** (“oblíqua” porque os eixos dimensionais não são perpendiculares, mas "distorcidos" pela correlação 
entre os fatores).



## Rotação Oblíqua {.scrollable}

- Uma rotação oblíqua comum é a rotação `oblimin` do pacote GPArotation.

```{r}
library(GPArotation)

# modelo fatorial com rotação oblíqua
(brand_fa_ob <- factanal(brand_df[, 1:9], factors = 3, rotation = "oblimin"))
```



## Rotação Oblíqua {.scrollable}

Quando comparamos o resultado da rotação `oblimin` com a rotação `varimax`, 
há duas diferenças principais:

1. As cargas fatoriais são ligeiramente diferentes para os relacionamentos
dos fatores com os adjetivos. No entanto, as cargas são suficientemente semelhantes para que não haja mudança substancial na forma como 
interpretaríamos os factores. Há ainda fatores para “valor,” “líder,” e 
“mais recente”.

2. O resultado inclui uma matriz de correlação de fatores que mostra as 
relações entre os fatores latentes estimados. O Fator 1 (valor) 
é negativamente correlacionado com o Fator 2 (líder), r = −0,39, e é essencialmente não correlacionado com o Fator 3 (mais recente),
r = 0,037.



## Rotação Oblíqua 

- A correlação negativa entre os fatores 1 e 2 é consistente com a teoria 
de que marcas "líderes" tendem a não serem marcas de "valor" e, portanto, 
pensamos que este é um resultado mais interpretável.

- No entanto, em outros casos, uma rotação correlacionada pode, ou não, ser 
uma solução melhor do que uma rotação ortogonal. 

- Esta é em grande parte uma questão a ser decidida com base no conhecimento do domínio e na utilidade interpretativa, e não em Estatística.



## EFA: Visualização

- As cargas fatoriais são exibidas no resultado. 

- No objeto `brand_fa_ob`, elas estão armazenadas no elemento 
`brand_fa_ob$loadings`. 

- Podemos visualizar o relacionamento fator-item com um mapa de calor das 
cargas.



## EFA: Visualização

```{r}
#| output-location: slide
library(gplots)
library(RColorBrewer)

heatmap.2(
  brand_fa_ob$loadings,
  col = brewer.pal(9, "Blues"),
  trace = "none",
  key = FALSE,
  dend = "none",
  Colv = FALSE,
  cexCol = 1.2,
  main = "\n\n\n\n\nCargas Fatorias para os Adjetivos das Marcas"
)
```



## EFA: Visualização

- O mapa de calor mostra uma separação distinta de itens em três fatores, que podem ser interpretados aproximadamente como "valor", "líder" e "mais recente".

- Observe que o item `rebuy`, que reflete a intenção declarada de recompra, 
possui cargas relevantes tanto no Fator1 (valor) quanto no Fator2 (líder).

- Isto sugere que, com base nesses dados, os consumidores indicam que 
recomprariam uma marca tanto por ter um bom valor ou porque é líder.



## EFA: Visualização

- Outro gráfico útil para modelos de análise fatorial é um *path diagram*, que mostra as variáveis latentes e os itens individuais que possuem altas 
cargas associadas às variáveis latentes.

- O função `semPaths()` do pacote semPlot produz uma representação visual de um modelo de análise fatorial.

```{r}
#| output-location: slide
library(semPlot)

semPaths(brand_fa_ob,
  what = "est", residuals = FALSE,
  cut = 0.3, posCol = c("white", "darkgreen"), negCol = c("white", "red"),
  edge.label.cex = 0.75, nCharNodes = 7
)
```



## Função `semPlot::semPaths()` {.scrollable}

- Plotamos o modelo `brand_fa_ob` ajustado anteriormente.

- Para plotar as estimativas das cargas, usamos `what = "est"`. 

- Omitimos as estimativas residuais para variáveis manifestas usando 
`residuals = FALSE`.

- Em seguida, cortamos cargas com magnitude absoluta < 0,3 adicionando 
`cut = 0.3` e as opções `posCol = c("white", "darkgreen")` e 
`negCol = c("white", "red")`.

- O argumento `posCol` faz com que cargas positivas < 0,3 sejam coloridas em branco (e, portanto, não aparecem na saída), enquanto cargas > 0,3 devem ser verdes escuras.

- O argumento `negCol` exclui ou usa vermelho para cargas < 0. 

- Ajustamos o tamanho do texto das cargas com `edge.label.cex` e criamos 
espaço para os nomes completos de variáveis com `nCharNodes`.



## EFA: Pesquisa sobre a Percepcão de Marcas

- No geral, o resultado da EFA para este conjunto de dados é que, em vez de utilizar 9 variáveis, poderíamos representar os dados com 3 fatores latentes subjacentes.

- Vimos que cada fator é mapeado para 2–4 das variáveis manifestas.

- No entanto, esta análise modelou apenas as relações das variáveis de classificação entre si. 

- Na próxima seção, usaremos os *scores* dos fatores estimados para 
aprendermos sobre as marcas.



## EFA: Scores dos Fatores

- Além de estimar a estrutura fatorial, a EFA também estima os *scores dos fatores latentes* para cada observação.

- Para nossa pesquisa, isto fornece as melhores estimativas das classificações latentes de cada consumidor para os fatores “valor”, “líder” e “mais recentes”.

- Podemos então usar os scores dos fatores para determinar as posições das 
marcas em relação aos fatores.

- A interpretação dos scores dos fatores permite nos concentrarmos em 
um conjunto menor e interpretável de dimensões que mapeiam construtos 
ao invés de itens individuais.



## EFA: Scores dos Fatores

```{r}
brand_fa_ob <- factanal(brand_df[, 1:9],
  factors = 3, rotation = "oblimin",
  scores = "Bartlett"
)
brand_scores <- data.frame(brand_fa_ob$scores)
brand_scores$brand <- brand_df$brand
head(brand_scores)
```

- Obtemos os scores dos fatores usando o argumento `scores = ....`

- Utilizamos os scores de Bartlett e os extraímos do
objeto `factanal()` com `brand_fa_ob$scores`, e armazenamos os scores 
na data frame `brand_scores`.



## EFA: Scores dos Fatores

- O resultado são scores estimados para cada respondente em cada fator e marca. 

- Se desejarmos investigar correlações dos fatores com dados demográficos ou comportamento de compra, poderíamos usar essas estimativas dos scores dos
fatores.

- Isto pode ser muito útil em análises como regressão e segmentação porque 
reduz a complexidade do modelo (número de dimensões), pois ao invés de 9 
itens, temos três fatores.



## EFA: Scores dos Fatores {.scrollable}

Para encontrar a posição geral de uma marca, odemos usar a função 
`aggregate()` para calcular as avaliações médias agregadas por marca, 
ou seja, a média de cada variável (adjetivo) por marca.

```{r}
# calulca avaliacoes medias agregadas por marca
brand_fa_media <- aggregate(. ~ brand, data = brand_scores, mean)

# nomes das marcas como nome das linhas,
rownames(brand_fa_media) <- brand_fa_media[, 1]

# elimina a primeira coluna contendo os nomes
brand_fa_media <- brand_fa_media[, -1]

# nomeando as colunas com o nome dos fatores estimados
names(brand_fa_media) <- c("Leader", "Value", "Latest")
brand_fa_media
```



## EFA: Scores dos Fatores {.scrollable}

Finalmente, um mapa de calor pode representar graficamente os scores médios por marca:

```{r}
#| output-location: slide
#| fig-cap: "marcas similares: f e g, b e c, e assim por diante"
heatmap.2(as.matrix(brand_fa_media),
  col = brewer.pal(9, "GnBu"), trace = "none", key = FALSE, dend = "none",
  cexCol = 1.2, main = "\n\n\nScore Fatorial Médio por Marca"
)
```



## Conclusões {.scrollable}

- A análise fatorial é desafiadora com tantas opções de estimação e 
métodos de rotação, juntamente com quantidades gigantescas de resultados (A saída da função `factanal()` é bastante pequena em comparação a outros programas.) 

- No entanto, a decisão mais importante é o número de fatores. A escolha do método de estimação e rotação é menos crucial de acordo com @jw2007 . 

- Se uma análise fatorial for bem-sucedida, várias combinações de métodos de 
estimação e rotações devem resultar na mesma conclusão.

- Em uma nota decepcionante, @jw2007 também afirmam que a "a grande maioria das tentativas de análises fatoriais não produz resultados 
claros" (p. 526). 

- Não há garantia de que uma análise fatorial levará a uma descoberta satisfatória de fatores latentes significativos. 

- Se você se ficar intrigado com os resultados de uma análise fatorial porque 
não parecia "funcionar", há uma boa chance de você não ter feito nada de 
errado e que a análise fatorial simplesmente não tenha encontrado nada interessante. 

- Um bom lugar para começar é examinar a matriz de correlação dos seus dados. 
Se houver poucas ou nenhuma correlações altas, realmente não adianta 
tentar uma análise fatorial.

- Podemos concluir que a Análise Fatorial Exploratória é uma forma útil de examinar a estrutura subjacente e o relacionamento entre as variáveis. 

- Quando os itens estão relacionados aos construtos subjacentes, a EFA 
pode reduzir a complexidade dos dados, agregando variáveis para criar criar variáveis latentes mais simples e interpretáveis.


## Referências

::: {#refs}
:::












