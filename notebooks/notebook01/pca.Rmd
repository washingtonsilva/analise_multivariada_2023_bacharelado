---
title: 'Lab: Análise de Componentes Principais'
output:
  html_document:
    df_print: paged
---

## Análise de Componentes Principais com base R

Neste lab, aplicaremos PCA nos dados `USArrests`, que faz parte do pacote base `R`. 

As linhas da data frame contêm os 50 estados, em ordem alfabética:

```{r chunk1}
estados <- row.names(USArrests)
estados
```

As colunas da data frame contêm as quatro variáveis:

```{r chunk2}
names(USArrests)
```

Inicialmente, examinamos brevemente os dados. Podemos notar que as variáveis 
têm médias muito diferentes.

```{r chunk3}
apply(USArrests, 2, mean)
```

Observe que a função `apply()` nos permite aplicar uma função --- neste caso, a função `mean()` --- a cada linha ou coluna do conjunto de dados. 

O segundo argumento aqui indica se desejamos calcular a média das linhas, 
$1$, ou das colunas, $2$. 

Vemos que há, em média, três vezes mais violações do que homicídios, e 
mais de oito vezes mais agressões do que violações.

Também podemos examinar as variância das quatro variáveis usando a função `apply()`.

```{r chunk4}
apply(USArrests, 2, var)
```

Não surpreende que a variabilidade muito diferente.

A variável `UrbanPop` mede a porcentagem da população de cada estado que vive em área urbana, que não é um número comparável ao número de estupros
em cada estado por 100.000 indivíduos.

Se não padronizamos as variáveis antes de realizar a PCA, então a maioria 
dos componentes principais que observamos seriam impulsionados pela variável `Assault`, uma vez que ela tem de longe as maiores média e variância.

Assim, é importante padronizar as variáveis para que tenham média zero e desvio padrão um antes de realizar a PCA.

Agora realizamos a análise de componentes principais usando a função `prcomp()`, que é uma das várias funções em `R` que executam PCA:

```{r chunk5}
pr.out <- prcomp(USArrests, scale = TRUE)
```

Por padrão, a função `prcomp()` centraliza as variáveis para terem média zero. Usando a opção `scale = TRUE`, escalamos o variáveis para que tenham desvio padrão igual a um. 

A saída de `prcomp()` contém uma série de quantidades úteis.

```{r chunk6}
names(pr.out)
```

Os componentes `center` e `scalce` correspondem às médias e desvios padrão das variáveis que foram utilizadas para escalonamento antes da implementação da
PCA.

```{r chunk7}
pr.out$center
pr.out$scale
```

A matriz `rotation` contém as cargas dos componentes principais; cada coluna de `pr.out$rotation` contém o correspondente vetor de cargas do componente principal.

**Obs.** Esta função é chamada de matriz de rotação, porque quando
multiplicamos a matriz $\bf X$ pela matriz `pr.out$rotation`, obtemos as coordenadas dos dados no sistema de coordenadas rotacionado. Essas 
coordenadas são os scores dos componentes principais.

```{r chunk8}
pr.out$rotation
```

Vemos que existem quatro componentes principais distintos. Isso era esperado 
porque em geral, existem $\min(n-1,p)$ componentes principais informativos em 
um conjunto de dados com $n$ observações e $p$ variáveis.

Usando a função `prcomp()`, não precisamos multiplicar explicitamente os dados pelos vetores de cargas do componente principal para obter os vetores de scores do componente principal. 

Em vez disso, a matriz `x` ($50 \times 4$) tem como colunas os vetores de 
scores dos componentes principais. Ou seja, a $k$ésima coluna de `x` é 
o $k$ésimo vetor de score do componente principal.

```{r chunk9}
dim(pr.out$x)
```

Podemos representar graficamente os dois primeiros componentes principais da seguinte forma:

```{r chunk10}
biplot(pr.out, scale = 0)
```

O argumento `scale = 0` de `biplot()` garante que as setas sejam dimensionadas para representar as cargas; outros valores para `scale` fornecem biplots ligeiramente diferentes com interpretações diferentes.

Observe que esta figura é uma imagem espelhada da Fig. 2 dos slides. 

Lembre-se de que os componentes principais só são únicos à exceção de uma 
mudança de sinal, portanto podemos reproduzir a Fig. 2 fazendo algumas 
pequenas alterações:

```{r chunk11, out.width = "100%"}
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)
```

A função `prcomp()` também gera o desvio padrão de cada componente principal. 

Por exemplo, para os dados `USArrests`, podemos acessar esses desvios padrão 
com:

```{r chunk12}
pr.out$sdev
```

A variância explicada por cada componente principal é obtida elevando os 
desvios padrão ao quadrado:

```{r chunk13}
pr.var <- pr.out$sdev^2
pr.var
```

Para calcular a proporção da variância explicada por cada componente principal, simplesmente dividimos a variância explicada por cada componente principal pela variância total explicada por todos os quatro componentes principais:

```{r chunk14}
pve <- pr.var / sum(pr.var)
pve
```

Vemos que o primeiro componente principal explica $62,0\,\%$ da variação nos dados, o segundo componente principal explica $24,7\,\%$ da variação e assim por diante.

Podemos plotar a PVE explicada por cada componente, bem como a PVE acumulada, 
fazendo:

```{r chunk15}
par(mfrow = c(1, 2))

plot(pve, 
     xlab = "Componente Principal",
     ylab = "Proporção oda Variância Explicada", 
     ylim = c(0, 1),
     type = "b")

plot(cumsum(pve), 
     xlab = "Componente Principal",
     ylab = "Proporção Acumulada da Variância Explicada",
     ylim = c(0, 1), 
     type = "b")
```

Observe que a função `cumsum()` calcula a soma acumulada dos elementos de um vetor numérico. Por exemplo:

```{r chunk16}
a <- c(1, 2, 8, -3)
cumsum(a)
```


## PCA via Decomposição SVD 

Para ilustar a aplicação da decomposição SVD para os cálculos da análise de componentes principais, vamos transformar a data frame `USArrests` em uma 
matriz, após padronizar cada coluna para ter média zero e variância um.

```{r chunk17}
X <- data.matrix(scale(USArrests))
pcob <- prcomp(X)
summary(pcob)
```

Vemos que o primeiro componente principal explica $62\%$ da variabilidade 
dos dados.

Vimos nos slides que resolver o problema de otimização para auma 
matriz de dados centrada $\bf X$ é equivalente a computar os primeiros $M$ 
componentes principais dos dados.  

A decomposição em valor singular (SVC) é um algoritmo geral para 
resolver a Eq. 12.6:

```{r chunk18}
sX <- svd(X)
names(sX)
round(sX$v, 3)
```

A função `svd()` retorna os três componentes da decomposição: `u`, `d`, e `v`. 

A matriz `v` é equivalente à matriz de cargas dos componentes principais 
(à exececão de uma inversão de sinal sem importância).

```{r chunk19}
pcob$rotation
```

A matriz `u` é equivalente à matriz de scores *padronizados*, e os desvios 
padrão estão no vetor `d`.

Podemos recuperar os vetores de scores usando a saída de `svd()`. Eles são idênticos aos vetores de scores gerados pela função `prcomp()`.

```{r chunk20}
t(sX$d * t(sX$u))
pcob$x
```



